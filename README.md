ğŸ§ª A/B Experimentation Platform

Industry-Grade | Reproducible | Statistically Correct








ğŸš€ Overview

This repository contains a production-style A/B experimentation platform built to reflect how real analytics and data science teams operate in industry.

Unlike toy A/B test scripts, this system emphasizes:

Correct statistical assumptions

Clean, modular architecture

Config-driven experiments

Automated, executive-ready reporting

Reproducibility and auditability

Key principle:
If a statistical method is not valid for the data, it is explicitly disabled.

ğŸ—ï¸ System Architecture
High-Level Architecture
flowchart TD
    A[experiment.yaml] --> B[Pipeline Orchestrator]
    B --> C[Data Loader & Validator]
    C --> D[Frequentist Tests]
    D --> E[Effect Size & Power]
    E --> F[JSON Results]

    B -->|if valid| G[Bayesian A/B Testing]
    G --> F

    F --> H[PDF Report Generator]
    H --> I[Executive Report]

Folder Structure
ab-experimentation-platform/
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                # Immutable input data
â”‚   â”œâ”€â”€ interim/            # CUPED-ready layer
â”‚   â””â”€â”€ processed/          # Aggregated outputs
â”‚
â”œâ”€â”€ experiments/
â”‚   â”œâ”€â”€ config/             # YAML-driven experiments
â”‚   â””â”€â”€ results/            # JSON outputs
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ core/               # Data loading & validation
â”‚   â”œâ”€â”€ statistics/         # All statistical logic
â”‚   â”œâ”€â”€ reporting/          # PDF generation
â”‚   â””â”€â”€ pipeline/           # Orchestration only
â”‚
â”œâ”€â”€ reports/                # Final PDF reports
â”œâ”€â”€ templates/              # Report templates
â”œâ”€â”€ tests/                  # Extensible testing layer
â”‚
â””â”€â”€ README.md

Design Rationale

statistics/ contains zero I/O â†’ pure, testable math

pipeline/ contains zero math â†’ orchestration only

config/ drives behavior â†’ no hard-coded experiments

This separation mirrors production experimentation platforms.

â–¶ï¸ Running an Experiment
python -m src.pipeline.run_experiment


This single command:

Loads experiment.yaml

Validates input data

Runs frequentist tests

Computes effect size & power

(Conditionally) runs Bayesian inference

Writes JSON outputs

Generates a PDF report

ğŸ“Š Statistical Methodology
Frequentist Testing (Primary)

For each continuous metric:

Normality: Shapiroâ€“Wilk test

Variance: Leveneâ€™s test

Test selection:

Welchâ€™s t-test (default)

Mannâ€“Whitney U when assumptions fail

Effect size: Cohenâ€™s d

Power analysis:

Achieved power

Required sample size (target = 0.80)

flowchart LR
    A[Metric Data] --> B{Normal?}
    B -->|Yes| C[T-Test]
    B -->|No| D[Mannâ€“Whitney U]
    C --> E[Effect Size]
    D --> E
    E --> F[Power Analysis]

ğŸ§  Why Bayesian A/B Testing Was Disabled

Bayesian A/B testing was implemented but intentionally disabled for this dataset.

Reason (Statistical Integrity)

The data is aggregated daily metrics, not trial-level data.

Bayesian Binomial models require:

n = number of trials (users / impressions)

k = number of successes

This dataset contains:

Daily aggregates

Purchase totals exceeding daily rows

Running a Binomial Bayesian model here would be mathematically invalid.

flowchart TD
    A[Daily Aggregates] -->|Invalid| B[Binomial Bayesian Model]
    B --> C[Incorrect Inference âŒ]

    A -->|Correct| D[Frequentist Tests]
    D --> E[Valid Decisions âœ…]


Industry rule followed:
Do not force a model when assumptions are violated.

Bayesian testing can be re-enabled instantly when user-level or impression-level data is available.

ğŸ“„ Outputs
JSON (Machine-Readable)
experiments/results/frequentist_results.json


Includes:

Metric

Test used

p-value

Effect size

Achieved power

Required sample size

Significance flag

Ideal for dashboards, APIs, or automation.

PDF (Executive-Ready)
reports/ab_test_report.pdf


Contains:

Experiment overview

Metric-level statistical results

Practical interpretation

Final recommendation

Designed for non-technical stakeholders.

ğŸ’¼ Business Interpretation Framework

This platform answers three business questions:

Is the result real? â†’ Statistical significance

Is it meaningful? â†’ Effect size

Can we trust it? â†’ Power analysis

A result is not shipped unless all three are satisfied.

This prevents:

False positives

Premature launches

Costly misinterpretations

ğŸ§© Example Decision Logic
flowchart TD
    A[p < 0.05] --> B{Effect Size Meaningful?}
    B -->|No| C[Do Not Roll Out]
    B -->|Yes| D{Power >= 0.80?}
    D -->|No| E[Collect More Data]
    D -->|Yes| F[Approve Rollout]

ğŸ”® Future Extensions

CUPED variance reduction

Sequential testing / early stopping

Dashboard integration (Tableau / Power BI)

User-level Bayesian modeling

Experiment versioning & logging

ğŸ Why This Project Stands Out

âœ” Not a notebook
âœ” Not a toy script
âœ” Statistically correct
âœ” Config-driven
âœ” Production-style architecture

This project demonstrates how to run experiments correctly, not just how to compute p-values.

### Reporting Templates

The `templates/` directory is reserved for HTML-based report templates.

During development, the system initially supported HTML â†’ PDF rendering
(Jinja2 + WeasyPrint). Due to OS-level native dependencies on Windows,
the final implementation uses ReportLab for reliable, pure-Python PDF
generation.

The folder is intentionally retained to support future HTML-based
reporting without changing the project structure.

ğŸ“Œ Author Note

This system was built with real-world constraints in mind:
dependency issues, invalid assumptions, debugging, and trade-offs.

